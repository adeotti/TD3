{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a58af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python 3.10\n",
    "#!pip install gymnasium==1.1.1\n",
    "#!pip install gymnasium-robotics==1.3.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5b82dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box,Dict\n",
    "from gymnasium.vector import SyncVectorEnv\n",
    "import gymnasium_robotics\n",
    "gym.register_envs(gymnasium_robotics)\n",
    "\n",
    "import numpy as np\n",
    "import torch,sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from dataclasses import dataclass\n",
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29e557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Hypers:\n",
    "    num_env : int = 10\n",
    "    max_ep_steps = 50\n",
    "    lr = 3e-4\n",
    "    device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available()else \"cpu\"\n",
    "    )\n",
    "    warmups = 200\n",
    "    batchsize = 256\n",
    "    minibatch = 16\n",
    "    num_episode = 2000\n",
    "\n",
    "    gamma = .99\n",
    "    tau = 0.005\n",
    "\n",
    "hypers = Hypers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb8960",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FetchReachCustom(gym.Wrapper):\n",
    "    def __init__(self,env : gym.Env):\n",
    "        super().__init__(env)\n",
    "        self.action_space = Box(-1,1,(3,),np.float32)\n",
    "        self.observation_space = Dict(\n",
    "            {\n",
    "            \"observation\" : Box(-np.inf,np.inf,(3,),np.float64),\n",
    "            \"achieved_goal\" : Box(-np.inf,np.inf,(3,),np.float64),\n",
    "            \"desired_goal\" : Box(-np.inf,np.inf,(3,),np.float64)\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def process_obs(self,observation):\n",
    "        observation[\"observation\"] = observation[\"observation\"][:3]\n",
    "        return observation\n",
    "         \n",
    "    def step(self, action):\n",
    "        action = np.append(action,0)\n",
    "        observation, reward, done,truncated, info = self.env.step(action)\n",
    "        return  self.process_obs(observation), reward, done,truncated, info\n",
    "    \n",
    "    def reset(self,seed=None, options=None):\n",
    "        observation,info = self.env.reset(seed=seed,options=options)\n",
    "        return self.process_obs(observation),info\n",
    "\n",
    "def tranform_observation(observation_dict : Dict): # -> torch.Size([9])\n",
    "    observation = observation_dict.get(\"observation\")\n",
    "    current_pos = observation_dict.get(\"achieved_goal\")\n",
    "    target = observation_dict.get(\"desired_goal\")\n",
    "    assert observation.shape == target.shape, f\"{observation.shape},{target.shape}\"\n",
    "    output = np.concatenate((observation,current_pos,target),axis=-1)\n",
    "    return torch.from_numpy(output).to(device=hypers.device,dtype=torch.float32)\n",
    "\n",
    "def sync_env():\n",
    "    def make_env():\n",
    "        x = gym.make(\"FetchReach-v3\",max_episode_steps=hypers.max_ep_steps)\n",
    "        x = FetchReachCustom(x)\n",
    "        return x\n",
    "    return SyncVectorEnv([make_env for _ in range(hypers.num_env)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fee667",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.LazyLinear(256)\n",
    "        self.l2 = nn.LazyLinear(256)\n",
    "        self.l3 = nn.LazyLinear(256)\n",
    "        self.output = nn.LazyLinear(3)\n",
    "        self.optim = torch.optim.Adam(self.parameters(),hypers.lr)\n",
    "    \n",
    "    def forward(self,obs: Tensor):\n",
    "        obs = F.relu(self.l1(obs))\n",
    "        obs = F.relu(self.l2(obs))\n",
    "        obs = F.relu(self.l3(obs))\n",
    "        output = F.tanh(self.output(obs))\n",
    "        return output\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.LazyLinear(256)\n",
    "        self.l2 = nn.LazyLinear(256)\n",
    "        self.output = nn.LazyLinear(1)\n",
    "    \n",
    "    def forward(self,state: Tensor,action: Tensor):\n",
    "        x = torch.cat((state,action),-1)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "def init_weights(w):\n",
    "    if isinstance(w,nn.Linear):\n",
    "        torch.nn.init.orthogonal_(w.weight)\n",
    "        torch.nn.init.constant_(w.bias,0.0)\n",
    "\n",
    "rand_obs = lambda : torch.rand((1,9),dtype = torch.float32,device=hypers.device)\n",
    "rand_action = lambda : torch.rand((1,3),dtype = torch.float32,device=hypers.device)\n",
    "\n",
    "def init_networks(a: Actor,q1: Critic,q2: Critic):\n",
    "    a(rand_obs())\n",
    "    a.apply(init_weights)\n",
    "\n",
    "    q1(rand_obs(),rand_action())\n",
    "    q1.apply(init_weights)\n",
    "\n",
    "    q2(rand_obs(),rand_action())\n",
    "    q2.apply(init_weights)\n",
    "\n",
    "actor = Actor()\n",
    "critic1 = Critic()\n",
    "critic2 = Critic()\n",
    "\n",
    "init_networks(actor,critic1,critic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12120f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class replay_buffer:\n",
    "    def __init__(self,env: SyncVectorEnv,actor: Actor):\n",
    "        self.env = env\n",
    "        self.actor = actor\n",
    "        self.data = []\n",
    "        self.rew = deque(maxlen=hypers.num_env)\n",
    "        self.episode_reward = np.zeros(self.env.num_envs, dtype=np.float32)\n",
    "        self.counter = 0\n",
    "        self.random_action_num = 0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def rollout(self,batchsize):\n",
    "        obs,_ = self.env.reset()\n",
    "        curr_state  = tranform_observation(obs)\n",
    "        for _ in range(batchsize):\n",
    "            self.counter+=1\n",
    "            if self.counter <= hypers.warmups:\n",
    "                self.random_action_num+=1\n",
    "                action = self.env.action_space.sample()\n",
    "            else:\n",
    "                noise_rollout = torch.normal(0.0,0.1,size=(self.env.action_space.shape))\n",
    "                action = (self.actor(curr_state) + noise_rollout).cpu().numpy()\n",
    "    \n",
    "            next_state,reward,done,trunc,_ = self.env.step(action)\n",
    "            \n",
    "            for n in range(hypers.num_env):\n",
    "                self.episode_reward[n]+=reward[n]\n",
    "                if trunc[n]:\n",
    "                    self.rew.append(self.episode_reward[n])\n",
    "                    self.episode_reward[n] = 0\n",
    "                     \n",
    "            next_state = tranform_observation(next_state)\n",
    "            self.data.append(\n",
    "                (\n",
    "                    curr_state,\n",
    "                    torch.from_numpy(action),\n",
    "                    torch.from_numpy(reward).to(torch.float32),\n",
    "                    next_state,\n",
    "                    torch.from_numpy(trunc).to(torch.float32)\n",
    "                )\n",
    "            )\n",
    "            curr_state = next_state  \n",
    "            \n",
    "    def sample(self,sample):\n",
    "        output = random.sample(self.data,sample)\n",
    "        curr_state,action,reward,next_state,trunc = zip(*output)\n",
    "        Stack = lambda x : torch.stack(x)\n",
    "        return (\n",
    "            Stack(curr_state),\n",
    "            Stack(action),\n",
    "            Stack(reward),\n",
    "            Stack(next_state),\n",
    "            Stack(trunc)\n",
    "        )\n",
    "\n",
    "    def util(self):\n",
    "        return self.rew,self.random_action_num\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7eed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Training:\n",
    "    def __init__(self):\n",
    "        self.actor = actor\n",
    "        self.actor_target = deepcopy(self.actor)\n",
    "        \n",
    "        self.q1 = critic1\n",
    "        self.q1_target = deepcopy(self.q1)\n",
    "        self.q2 = critic2\n",
    "        self.q2_target = deepcopy(self.q2)\n",
    "        self.critic_optim = torch.optim.Adam(\n",
    "            list(self.q1.parameters()) + list(self.q2.parameters()),lr=hypers.lr\n",
    "        )\n",
    "        self.env = sync_env()\n",
    "        self.replay_buffer = replay_buffer(self.env,self.actor)\n",
    "        self.writter = SummaryWriter(\"data/\")\n",
    "        self.total_it = 0\n",
    "\n",
    "    def save(self,num):\n",
    "        checkpoint = {\n",
    "            \"actor state\" : self.actor.state_dict(),\n",
    "            \"actor optim\": self.actor.optim.state_dict(),\n",
    "            \"actor target\" : self.actor_target.state_dict(),\n",
    "\n",
    "            \"q1 state\" : self.q1.state_dict(),\n",
    "            \"q2 state\" : self.q2.state_dict(),\n",
    "            \"critic optim state\" : self.critic_optim.state_dict(),\n",
    "            \"q1 target state\" : self.q1_target.state_dict(),\n",
    "            \"q2 target state\":self.q2_target.state_dict()  \n",
    "        }\n",
    "        torch.save(checkpoint,f\"data/td3_{num}.pth\")\n",
    "    \n",
    "    def load(self,path=\"data/td3.pth\"):\n",
    "        checkpoint = torch.load(path,map_location=hypers.device)\n",
    "\n",
    "        self.actor.load_state_dict(checkpoint[\"actor state\"],strict=True)\n",
    "        self.actor.optim.load_state_dict(checkpoint[\"actor optim\"])\n",
    "        self.actor_target.load_state_dict(checkpoint[\"actor target\"])\n",
    "        \n",
    "        self.critic_optim.load_state_dict(checkpoint[\"critic optim state\"])\n",
    "        self.q1.load_state_dict(checkpoint[\"q1 state\"],strict=True)\n",
    "        self.q2.load_state_dict(checkpoint[\"q2 state\"],strict=True)\n",
    "        self.q1_target.load_state_dict(checkpoint[\"q1 target state\"])\n",
    "        self.q2_target.load_state_dict(checkpoint[\"q2 target state\"])\n",
    "        \n",
    "    def train(self,num_episode,batchsize,minibatch):\n",
    "\n",
    "        for traj in tqdm(range(int(num_episode)),total=hypers.num_episode):\n",
    "            self.replay_buffer.rollout(batchsize)\n",
    "\n",
    "            if self.replay_buffer.random_action_num >= hypers.warmups: # start training after collecor full with random data\n",
    "                assert len(self.replay_buffer) > hypers.warmups\n",
    "                self.total_it +=1\n",
    "                \n",
    "                for _ in range(batchsize//minibatch):\n",
    "                    s,a,r,ns,trunc = self.replay_buffer.sample(minibatch) \n",
    "                    # s : state, a : action, r : reward, ns : next state, trunc : truncated\n",
    "                    with torch.no_grad():\n",
    "                        noise_train = torch.normal(0.0,0.2,size=(hypers.minibatch,hypers.num_env,3)).clamp(-0.5,0.5)\n",
    "                        next_action = (self.actor_target(ns) + noise_train).clamp(-1,1)\n",
    "                        q1_target = self.q1_target(ns,next_action).squeeze(-1)\n",
    "                        q2_target = self.q2_target(ns,next_action).squeeze(-1)\n",
    "                        q_target = r + (1-trunc) * hypers.gamma * torch.min(q1_target,q2_target)\n",
    "                    \n",
    "                    q1 = self.q1(s,a).squeeze(-1)\n",
    "                    q2 = self.q2(s,a).squeeze(-1)\n",
    "                    critic_loss = F.mse_loss(q1,q_target) + F.mse_loss(q2,q_target)\n",
    "        \n",
    "                    self.critic_optim.zero_grad()\n",
    "                    critic_loss.backward()\n",
    "                    self.critic_optim.step()\n",
    "\n",
    "                    if self.total_it % 2 == 0: \n",
    "                        action = self.actor(s)\n",
    "                        actor_loss = -self.q1(s,action).mean()\n",
    "                        self.writter.add_scalar(\"main/Policy loss\",actor_loss.mean(),traj)\n",
    "\n",
    "                        self.actor.optim.zero_grad()\n",
    "                        actor_loss.backward()\n",
    "                        self.actor.optim.step()\n",
    "\n",
    "                        # polyak averaging\n",
    "                        for actor_param,actor_target_param in zip(self.actor.parameters(),self.actor_target.parameters()):\n",
    "                            actor_target_param.data.copy_(\n",
    "                                (hypers.tau * actor_param) + (1-hypers.tau) * actor_target_param\n",
    "                            )\n",
    "                        \n",
    "                        for q1_param,q1_target_param in zip(self.q1.parameters(),self.q1_target.parameters()):\n",
    "                            q1_target_param.data.copy_(\n",
    "                                hypers.tau*q1_param + (1-hypers.tau) * q1_target_param\n",
    "                            )\n",
    "                        \n",
    "                        for q2_param,q2_target_param in zip(self.q2.parameters(),self.q2_target.parameters()):\n",
    "                            q2_target_param.data.copy_(\n",
    "                                hypers.tau*q2_param + (1-hypers.tau) * q2_target_param\n",
    "                            )\n",
    "                    \n",
    "                if traj!=0 and traj%100 == 0:\n",
    "                    self.save(traj)\n",
    "                \n",
    "                self.writter.add_scalar(\"main/action variance\",a.var(),traj)\n",
    "                self.writter.add_scalar(\"main/next action variance\",next_action.var(),traj)\n",
    "                self.writter.add_scalar(\"main/Critic Loss\",critic_loss.mean(),traj)\n",
    "                self.writter.add_scalar(\"main/Q target\",q_target.mean(),traj)\n",
    "                self.writter.add_scalar(\"main/Epi reward\",torch.tensor(self.replay_buffer.util()[0]).mean())\n",
    "                self.writter.add_scalar(\"main/Random action collection\",self.replay_buffer.util()[1])\n",
    "\n",
    "\n",
    "t = Training()\n",
    "t.train(\n",
    "    hypers.num_episode,\n",
    "    hypers.batchsize,\n",
    "    hypers.minibatch\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
