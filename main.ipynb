{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a58af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python 3.10\n",
    "#!pip install gymnasium==1.1.1\n",
    "#!pip install gymnasium-robotics==1.3.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5b82dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box,Dict\n",
    "from gymnasium.vector import SyncVectorEnv\n",
    "import gymnasium_robotics\n",
    "gym.register_envs(gymnasium_robotics)\n",
    "\n",
    "import numpy as np\n",
    "import torch,sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from dataclasses import dataclass\n",
    "from copy import deepcopy\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29e557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Hypers:\n",
    "    num_env : int = 2\n",
    "    lr = 3e-4\n",
    "    device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available()else \"cpu\"\n",
    "    )\n",
    "    batchsize = 10\n",
    "    minibatch = 5\n",
    "    num_episode = 5\n",
    "\n",
    "    policy_noise = 0.2\n",
    "    noise_clip = 0.5\n",
    "    gamma = 0.4\n",
    "    tau = 1.5\n",
    "\n",
    "hypers = Hypers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dafb8960",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FetchReachCustom(gym.Wrapper):\n",
    "    def __init__(self,env : gym.Env):\n",
    "        super().__init__(env)\n",
    "        self.action_space = Box(-1,1,(3,),np.float32)\n",
    "        self.observation_space = Dict(\n",
    "            {\n",
    "            \"observation\" : Box(-np.inf,np.inf,(3,),np.float64),\n",
    "            \"achieved_goal\" : Box(-np.inf,np.inf,(3,),np.float64),\n",
    "            \"desired_goal\" : Box(-np.inf,np.inf,(3,),np.float64)\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def process_obs(self,observation):\n",
    "        observation[\"observation\"] = observation[\"observation\"][:3]\n",
    "        return observation\n",
    "         \n",
    "    def step(self, action):\n",
    "        action = np.append(action,0)\n",
    "        observation, reward, done,truncated, info = self.env.step(action)\n",
    "        return  self.process_obs(observation), reward, done,truncated, info\n",
    "    \n",
    "    def reset(self,seed=None, options=None):\n",
    "        observation,info = self.env.reset(seed=seed,options=options)\n",
    "        return self.process_obs(observation),info\n",
    "\n",
    "def tranform_observation(observation_dict : Dict): # -> torch.Size([6])\n",
    "    observation = observation_dict.get(\"observation\")\n",
    "    target = observation_dict.get(\"achieved_goal\")\n",
    "    assert observation.shape == target.shape\n",
    "    output = np.concatenate((observation,target),axis=-1)\n",
    "    return torch.from_numpy(output).to(device=hypers.device,dtype=torch.float32)\n",
    "\n",
    "def sync_env():\n",
    "    def make_env():\n",
    "        x = gym.make(\"FetchReach-v3\")\n",
    "        x = FetchReachCustom(x)\n",
    "        return x\n",
    "    return SyncVectorEnv([make_env for _ in range(hypers.num_env)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6fee667",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.LazyLinear(256)\n",
    "        self.l2 = nn.LazyLinear(256)\n",
    "        self.l3 = nn.LazyLinear(256)\n",
    "        self.l4 = nn.LazyLinear(256)\n",
    "        self.output = nn.LazyLinear(3)\n",
    "        self.optim = torch.optim.Adam(self.parameters(),hypers.lr)\n",
    "    \n",
    "    def forward(self,obs: Tensor):\n",
    "        obs = F.relu(self.l1(obs))\n",
    "        obs = F.relu(self.l2(obs))\n",
    "        obs = F.relu(self.l3(obs))\n",
    "        obs = F.relu(self.l4(obs))\n",
    "        output = F.tanh(self.output(obs))\n",
    "        return output\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.LazyLinear(256)\n",
    "        self.l2 = nn.LazyLinear(256)\n",
    "        self.output = nn.LazyLinear(1)\n",
    "        self.optim = torch.optim.Adam(self.parameters(),hypers.lr)\n",
    "    \n",
    "    def forward(self,state: Tensor,action: Tensor):\n",
    "        x = torch.cat((state,action),-1)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "def init_weights(w):\n",
    "    if isinstance(w,nn.Linear):\n",
    "        torch.nn.init.orthogonal_(w.weight)\n",
    "        torch.nn.init.constant_(w.bias,0.0)\n",
    "\n",
    "rand_obs = lambda : torch.rand((1,6),dtype = torch.float32,device=hypers.device)\n",
    "rand_action = lambda : torch.rand((1,3),dtype = torch.float32,device=hypers.device)\n",
    "\n",
    "def init_networks(a: Actor,q1: Critic,q2: Critic):\n",
    "    a(rand_obs())\n",
    "    a.apply(init_weights)\n",
    "\n",
    "    q1(rand_obs(),rand_action())\n",
    "    q1.apply(init_weights)\n",
    "\n",
    "    q2(rand_obs(),rand_action())\n",
    "    q2.apply(init_weights)\n",
    "\n",
    "actor = Actor()\n",
    "critic1 = Critic()\n",
    "critic2 = Critic()\n",
    "\n",
    "init_networks(actor,critic1,critic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12120f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class replay_buffer:\n",
    "    def __init__(self,env: SyncVectorEnv,actor: Actor):\n",
    "        self.env = env\n",
    "        self.actor = actor\n",
    "        self.data = []\n",
    "        self.pointer = 0\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def rollout(self,batchsize):\n",
    "        self.clear()\n",
    "        noise_rollout = torch.normal(0.0,0.1,size=(self.env.action_space.shape))\n",
    "        obs,_ = self.env.reset()\n",
    "        curr_state  = tranform_observation(obs)\n",
    "        for _ in range(batchsize):\n",
    "            action = (self.actor(curr_state) + noise_rollout).cpu().numpy()\n",
    "            next_state,reward,done,_,_ = self.env.step(torch.rand((2,3)).numpy())\n",
    "            next_state = tranform_observation(next_state)\n",
    "            self.data.append(\n",
    "                (\n",
    "                    curr_state,\n",
    "                    torch.from_numpy(action),\n",
    "                    torch.from_numpy(reward).to(torch.float32),\n",
    "                    next_state,\n",
    "                    torch.from_numpy(done).to(torch.float32)\n",
    "                )\n",
    "            )\n",
    "            curr_state = next_state  \n",
    "            \n",
    "    def sample(self,sample):\n",
    "        output = self.data[self.pointer:sample+self.pointer]\n",
    "        self.pointer+=sample\n",
    "        curr_state,action,reward,next_state,dones = zip(*output)\n",
    "        Stack = lambda x : torch.stack(x)\n",
    "        return (\n",
    "            Stack(curr_state),\n",
    "            Stack(action),\n",
    "            Stack(reward),\n",
    "            Stack(next_state),\n",
    "            Stack(dones)\n",
    "        )\n",
    "\n",
    "    def clear(self):\n",
    "        self.data = []\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7eed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self):\n",
    "        self.actor = actor\n",
    "        self.actor_target = deepcopy(self.actor)\n",
    "        \n",
    "        self.q1 = critic1\n",
    "        self.q1_target = deepcopy(self.q1)\n",
    "\n",
    "        self.q2 = critic2\n",
    "        self.q2_target = deepcopy(self.q2)\n",
    "\n",
    "        self.env = sync_env()\n",
    "        self.replay_buffer = replay_buffer(self.env,self.actor)\n",
    "    \n",
    "    def save(self):\n",
    "        checkpoint = {\n",
    "            \"actor state\" : self.actor.state_dict(),\n",
    "            \"actor optim\": self.actor.optim.state_dict(),\n",
    "            \"actor target\" : self.actor_target.state_dict(),\n",
    "\n",
    "            \"q1 state\" : self.q1.state_dict(),\n",
    "            \"q1 optim\": self.q1.optim.state_dict(),\n",
    "            \"q1 target state\" : self.q1_target.state_dict(),\n",
    "\n",
    "            \"q2 state\" : self.q2.state_dict(),\n",
    "            \"q2 optim\" : self.q2.optim.state_dict(),\n",
    "            \"q2 target state\":self.q2_target.state_dict()  \n",
    "        }\n",
    "        torch.save(checkpoint,\"./td3.pth\")\n",
    "    \n",
    "    def load(self,path):\n",
    "        checkpoint = torch.load(path,map_location=hypers.device)\n",
    "\n",
    "        self.actor.load_state_dict(checkpoint[\"actor state\"],strict=True)\n",
    "        self.actor.optim.load_state_dict(checkpoint[\"actor optim\"])\n",
    "        self.actor_target.load_state_dict(checkpoint[\"actor target\"])\n",
    "\n",
    "        self.q1.load_state_dict(checkpoint[\"q1 state\"],strict=True)\n",
    "        self.q1.optim.load_state_dict(checkpoint[\"q1 optim\"])\n",
    "        self.q1_target.load_state_dict(checkpoint[\"q1 target state\"])\n",
    "\n",
    "        self.q2.load_state_dict(checkpoint[\"q2 state\"],strict=True)\n",
    "        self.q2.optim.load_state_dict(checkpoint[\"q2 optim\"])\n",
    "        self.q2_target.load_state_dict(checkpoint[\"q2 target state\"])\n",
    "        \n",
    "\n",
    "    def train(self,num_episode,batchsize,minibatch):\n",
    "        for traj in range(num_episode):\n",
    "            self.replay_buffer.rollout(batchsize)\n",
    "            for _ in range(batchsize//minibatch):\n",
    "                curr_state,action,reward,next_state,done = self.replay_buffer.sample(minibatch)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    noise_train = torch.normal(0.0,0.2,size=(minibatch,3)).clamp(-0.5,0.5)\n",
    "                    next_action = (self.actor_target(next_state) + noise_train).clamp(-1,1)\n",
    "\n",
    "                    q1_target = self.q1_target(next_state,next_action)\n",
    "                    q2_target = self.q2_target(next_state,next_action)\n",
    "                    q_target = reward + (1-done) * hypers.gamma * torch.min(q1_target,q2_target)\n",
    "                 \n",
    "                q1 = self.q1(curr_state,action)\n",
    "                q2 = self.q2(curr_state,action)\n",
    "                critic_loss = F.mse_loss(q1,q_target) + F.mse_loss(q2,q_target)\n",
    "\n",
    "                self.q1.optim.zero_grad()\n",
    "                self.q2.optim.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                self.q1.optim.step()\n",
    "                self.q2.optim.step()\n",
    "\n",
    "                if traj % 2 == 0:\n",
    "                    x = self.actor(curr_state)\n",
    "                    actor_loss = -self.q1(curr_state,x).mean()\n",
    "                    self.actor.optim.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    self.actor.optim.step()\n",
    "\n",
    "                    # polyak averaging\n",
    "                    for actor_param,actor_target_param in zip(self.actor.parameters(),self.actor_target.parameters()):\n",
    "                        actor_target_param.data.copy_(\n",
    "                            (hypers.tau * actor_param) + (1-hypers.tau) * actor_target_param\n",
    "                        )\n",
    "                    \n",
    "                    for q1_param,q1_target_param in zip(self.q1.parameters(),self.q1_target.parameters()):\n",
    "                        q1_target_param.data.copy_(\n",
    "                            hypers.tau*q1_param + (1-hypers.tau) * q1_target_param\n",
    "                        )\n",
    "                    \n",
    "                    for q2_param,q2_target_param in zip(self.q2.parameters(),self.q2_target.parameters()):\n",
    "                        q1_target_param.data.copy_(\n",
    "                            hypers.tau*q2_param + (1-hypers.tau) * q2_target_param\n",
    "                        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
